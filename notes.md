# GPT-Engineer with Google Gemini-2.0-flash-exp

- google_custom_api="https://generativelanguage.googleapis.com"

### Export env vars to mimmic openai ones
As GPT-Engineer is set by default to work with Openai by just exporting Open Key in the virtual environment and the model GPT4o
we will just try to override the base url and use the gemini model

```bash
export OPENAI_API_BASE="https://generativelanguage.googleapis.com"
export OPENAI_API_KEY="***************************"
export MODEL_NAME="gemini-2.0-flash-exp"
export LOCAL_MODEL=true
```

### Setup google token to be used for api calls
Before anything you need to go to google developer platform:
- search for `Generative Language API` and activate it
- Then create service account and keys that are downloaded in a json file
- then use the api key to generate an authentication token that can be used through the url to call google gemini
- use this python app with you api key to get that token
```python
# step one installtions:
pip install google-auth google-auth-oauthlib google-auth-httplib2 python-dotenv
# make sure that you have your credentials downloaded from google Service Account created in the .json file
# use those in the python app
import os
from google.oauth2 import service_account
import google.auth.transport.requests
from dotenv import load_dotenv


load_dotenv()

SCOPES = ["https://www.googleapis.com/auth/generative-language"]
SERVICE_ACCOUNT_FILE_PATH = os.getenv("SERVICE_ACCOUNT_JSON_CREDENTIALS_PATH")

credentials = service_account.Credentials.from_service_account_file(
    SERVICE_ACCOUNT_FILE_PATH, scopes=SCOPES
)

auth_request = google.auth.transport.requests.Request()
credentials.refresh(auth_request)
print("Access Token:", credentials.token)

# helper that will find the env var and change its content for openai api key to be replaced by the token generated by this script
def update_env_var(file_path, var_name, new_value, add_if_missing=True):
    """
    Updates the value of an environment variable in a .env file.

    Parameters:
        file_path (str): Path to the .env file.
        var_name (str): Name of the environment variable to update.
        new_value (str): New value for the environment variable.
        add_if_missing (bool): If True, adds the variable if it doesn't exist.
    
    Returns:
        bool: True if the variable was updated or added, False otherwise.
    """
    try:
        with open(file_path, 'r') as file:
            lines = file.readlines()
        
        updated = False
        for i, line in enumerate(lines):
            if line.strip().startswith(f"{var_name}="):
                lines[i] = f"{var_name}={new_value}\n"
                updated = True
                break

        if not updated and add_if_missing:
            lines.append(f"{var_name}={new_value}\n")
            updated = True

        with open(file_path, 'w') as file:
            file.writelines(lines)

        return updated
    except FileNotFoundError:
        print(f"Error: File not found: {file_path}")
        return False
    except Exception as e:
        print(f"An error occurred: {e}")
        return False


if update_env_var(".env", OPENAI_API_KEY, credentials.token):
    print(f"Successfully updated {var_name} in {file_path} with value ************** etc...")
else:
    print(f"Failed to update {var_name} in {file_path}.")

```

# **ALL ABOVE WORKED FINE AND GOT ENV VAR POPULATED BUT THE GOOGLE API URL IS NOT OPENAI COMPATIBLE, THEREFORE, TRYING WITH GROQ**

# GROQ way
- set env var in a .env file :
  - OPENAI_BASE_URL="https://api.groq.com/openai/v1"
  - OPENAI_API_KEY="<groq_api_key>" 
- start using:
```bash
# used model `llama-3.3-70b-versatile` and worked fine
gpte --lite --temperature=0.1 --model=<groq_model>
```


# NOw LLAMA.CCP way so that we can get any Hugggingface GGUF model to run
- install
```python
pip install llama-cpp-python
pip install 'llama-cpp-python[server]'
```
- download a GGUF model from hugging face
```bash
mkdir models
cd models
# mistral-7b from TheBloke
wget https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF/resolve/main/capybarahermes-2.5-mistral-7b.Q5_0.gguf
```
- Test if model works setting up the server inference that will host the model listening for calls
```bash
# export env vars
```bash
export OPENAI_API_BASE="http://localhost:8000/v1"
export OPENAI_API_KEY="noneed"
export MODEL_NAME="capybarahermes-2.5-mistral-7b.Q5_0.gguf"
export LOCAL_MODEL=true
```
# in terminal 1: server
python -m llama_cpp.server --model /home/creditizens/gpt_engineer/models/capybarahermes-2.5-mistral-7b.Q5_0.gguf --n_batch 256
# in terminal 2: client calling this one
gpte <project_folder> --model=capybarahermes-2.5-mistral-7b.Q5_0.gguf --lite --temperature=0.1
gpte . --model=capybarahermes-2.5-mistral-7b.Q5_0.gguf --lite --temperature=0.1

```
























